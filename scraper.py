import urllib.request as req
import pandas as pd
import bs4

# Goodinfo篩選頁
url = "https://goodinfo.tw/tw2/StockList.asp?MARKET_CAT=%E8%87%AA%E8%A8%82%E7%AF%A9%E9%81%B8&INDUSTRY_CAT=%E6%88%91%E7%9A%84%E6%A2%9D%E4%BB%B6&FL_ITEM0=%E6%88%90%E4%BA%A4%E5%BC%B5%E6%95%B8+%28%E5%BC%B5%29&FL_VAL_S0=1000&FL_VAL_E0=&FL_ITEM1=%E6%98%A8%E6%97%A5%E6%88%90%E4%BA%A4%E5%BC%B5%E6%95%B8+%28%E5%BC%B5%29&FL_VAL_S1=1000&FL_VAL_E1=&FL_ITEM2=&FL_VAL_S2=&FL_VAL_E2=&FL_ITEM3=&FL_VAL_S3=&FL_VAL_E3=&FL_ITEM4=&FL_VAL_S4=&FL_VAL_E4=&FL_ITEM5=&FL_VAL_S5=&FL_VAL_E5=&FL_ITEM6=&FL_VAL_S6=&FL_VAL_E6=&FL_ITEM7=&FL_VAL_S7=&FL_VAL_E7=&FL_ITEM8=&FL_VAL_S8=&FL_VAL_E8=&FL_ITEM9=&FL_VAL_S9=&FL_VAL_E9=&FL_ITEM10=&FL_VAL_S10=&FL_VAL_E10=&FL_ITEM11=&FL_VAL_S11=&FL_VAL_E11=&FL_RULE0=%E7%94%A2%E6%A5%AD%E9%A1%9E%E5%88%A5%7C%7CETF&FL_RULE_CHK0=T&FL_RULE1=&FL_RULE2=&FL_RULE3=&FL_RULE4=&FL_RULE5=&FL_RANK0=&FL_RANK1=&FL_RANK2=&FL_RANK3=&FL_RANK4=&FL_RANK5=&FL_FD0=&FL_FD1=&FL_FD2=&FL_FD3=&FL_FD4=&FL_FD5=&FL_SHEET=%E4%BA%A4%E6%98%93%E7%8B%80%E6%B3%81&FL_SHEET2=%E6%97%A5&FL_MARKET=%E5%8F%AA%E6%9C%89%E4%B8%8A%E5%B8%82&FL_QRY=%E6%9F%A5++%E8%A9%A2"
request = req.Request(url,headers = {
    "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36",
    "Cookie" :  "_ga_F0NLZ0KD98=GS1.1.1659761203.1.1.1659762119.54; _ga_MXXGNT05SQ=GS1.1.1678155140.1.0.1678155140.0.0.0; AMCV_4D6368F454EC41940A4C98A6%40AdobeOrg=179643557%7CMCIDTS%7C19627%7CMCMID%7C13927912176595077714100628360762012642%7CMCAAMLH-1696332474%7C11%7CMCAAMB-1696332474%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1695734874s%7CNONE%7CMCAID%7CNONE%7CMCCIDH%7C1138916753%7CvVersion%7C5.5.0; s_pers=%20v8%3D1695727787847%7C1790335787847%3B%20v8_s%3DFirst%2520Visit%7C1695729587847%3B%20c19%3Dsd%253Apdfft%253Apdf%253Aurl%7C1695729587850%3B%20v68%3D1695727786814%7C1695729587856%3B; _ga_FV57JP1BX2=GS1.1.1696920592.2.1.1696922276.0.0.0; _ga=GA1.1.1056148151.1631340020; _ga_MLQVRK2V4D=GS1.1.1705637850.13.1.1705640034.0.0.0; _ga_QRNK9NMFG3=GS1.1.1705635492.1.1.1705640704.0.0.0; _ga_G5SML9BFJJ=GS1.1.1706683869.20.1.1706684185.0.0.0; _ga_KBRVS3K3RP=GS1.1.1708513057.1.1.1708513541.0.0.0; nukcourse-47873=BHOKHPIMLLAB; ASPSESSIONIDCUQCADQT=MHFACIICHNMDNIKDHJJPCEEI"
})
with req.urlopen(request) as response:
    Data = response.read().decode("utf-8")
# 取得HTML資訊
HTML = bs4.BeautifulSoup(Data,"lxml")
# 取得股票資訊表格
tblStockList = HTML.select("#tblStockList")
row = tblStockList[0]
# 依序讀取"代號"及"名稱"兩欄位
pd_table = pd.read_html(row.prettify())[0][["代號","名稱"]]
# 移除重複的標題欄位
pd_table = pd_table.drop(pd_table[pd_table["代號"]=="代號"].index)
# 存檔
pd_table.to_csv('readStockList.csv', index=False) 
